# -*- coding: utf-8 -*-
"""Copy of Elyse copy of Proj w Twitter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1miQqjz-GaEBVARGveIdyxMLXaGRJA9W6
"""

!pip install transformers
!pip install datasets
!pip install huggingface_hub

from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Initialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base")
# Initialize the model
model = AutoModelForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base", num_labels = 3)

!pip install python-twitter
import twitter
from datasets import load_dataset, DatasetDict
from huggingface_hub import notebook_login

notebook_login()
#dataset = load_dataset("chiarab/covid-tweet-sentiment", use_auth_token=True)

consumer_key = 'ukc4AdhfcaGaDltF3tBiLi8jQ'
consumer_key_secret = 'mHpOF3VuWpCCubYlgLaWLd8wGYOu625pO5XMLHw4FXebZbcrxr'
access_token = '1156238956779986947-ygcAES31ByJDSXjX9XNI4Fr94kfhUw'
access_token_secret = 'ckEW3z6oWa5PB9SiHjgJI2fkg9lLnGkjVwKHKxmWwTl7i'

api = twitter.Api(consumer_key=consumer_key,
                  consumer_secret=consumer_key_secret,
                  access_token_key=access_token,
                  access_token_secret=access_token_secret,
                  sleep_on_rate_limit=True)

tweets = load_dataset('chiarab/covid-tweet-sentiment', use_auth_token=True)

def add_tweet_content(example):
    try:
        status = api.GetStatus(example["tweet_ID"])
        return {"id": status.text}
    except twitter.TwitterError as err:
        return {"id": ""}

def add_tweet_content_tiny(example):
    try:
        #status = api.GetStatus(example["tweet_ID"])
        status = api.GetStatus(example)
        return {"id": status.text}
    except twitter.TwitterError as err:
        return {"id": ""}

add_tweet_content_tiny(1392432249522512000)

# Take random examples for train and validation
finetune_train = tweets['train'].shuffle(seed=1111).select(range(1800))
finetune_val = tweets['train'].shuffle(seed=1111).select(range(1800, 1915))
finetune_test = tweets['train'].shuffle(seed=1111).select(range(1915, 2029))

finetune_train = finetune_train.map(add_tweet_content)
finetune_val = finetune_val.map(add_tweet_content)
finetune_test = finetune_test.map(add_tweet_content)

print(finetune_test)
print(type(finetune_val['tweet_ID'][1]))
print(finetune_val['tweet_ID'])
print(finetune_val)
print(finetune_test['id'])
print(finetune_val['id'])
print(finetune_train['id'])

from torch.utils.data import DataLoader

train_finetune_dataloader = DataLoader(finetune_train, batch_size=16)
eval_finetune_dataloader = DataLoader(finetune_val, batch_size=16)

def get_target(val):
  vec = np.zeros((1,3))
  vec[0][val] = 1
  return torch.from_numpy(vec)

import numpy as np
import torch
from transformers import AdamW, get_linear_schedule_with_warmup
from tqdm.notebook import tqdm

num_epochs = 3
num_training_steps = 3 * len(train_finetune_dataloader)
optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)
lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)

best_val_loss = float("inf")
progress_bar = tqdm(range(num_training_steps))
for epoch in range(num_epochs):
    # training
    model.train()
    for batch_i, batch in enumerate(train_finetune_dataloader):
      input = batch['id'][1]
      labels = get_target(batch['attitude'])
      inputs = tokenizer(input, return_tensors="pt")
      outputs = model(**inputs, labels=labels)
      outputs.loss.backward()
      optimizer.step()
      lr_scheduler.step()
      progress_bar.update(1)

    # validation
    loss = 0
    model.eval()
    for batch_i, batch in enumerate(eval_finetune_dataloader):
        with torch.no_grad():
          input = batch['id'][0]
          labels = get_target(batch['attitude'])
          inputs = tokenizer(input, return_tensors="pt")
          outputs = model(**inputs, labels=labels)
        loss += outputs.loss
    
    avg_val_loss = loss / len(finetune_val)
    print(f"Validation loss: {avg_val_loss}")
    if avg_val_loss < best_val_loss:
        print("Saving checkpoint!")
        best_val_loss = avg_val_loss
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_loss': best_val_loss,
            },
            f"checkpoints/epoch_{epoch}.pt"
        )

from google.colab import drive
drive.mount('/content/drive')

correct = 0
wrong = 0

model.eval()
for tweet in finetune_val:
  inputs = tweet['id']
  tokenized_inputs = tokenizer(inputs, return_tensors="pt")
  outputs = model(**tokenized_inputs)
  labels = [0, 1, 2]
  prediction = torch.argmax(outputs.logits)
  if prediction == tweet['attitude']:
    correct += 1
  else:
    wrong += 1

print("Finetuned Accuracy:")
print(correct / (correct + wrong))

correct = 0
wrong = 0

# Initialize the regular tokenizer
tokenizer2 = AutoTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base")
# Initialize the regular model
model2 = AutoModelForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base", num_labels = 3)

model2.eval()
for tweet in finetune_val:
  inputs = tweet['id']
  tokenized_inputs = tokenizer2(inputs, return_tensors="pt")
  outputs = model2(**tokenized_inputs)
  labels = [0, 1, 2]
  prediction = torch.argmax(outputs.logits)
  if prediction == tweet['attitude']:
    correct += 1
  else:
    wrong += 1

print("Regular Accuracy:")
print(correct / (correct + wrong))
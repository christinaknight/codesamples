# -*- coding: utf-8 -*-
"""Copy of chiara train with10extra_training_model_CS224Nfinalproj_march12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bWnTMtLxwYIK5_vuuCChG9iJiWPxihAs
"""

!pip install transformers
!pip install datasets
!pip install huggingface_hub

from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Initialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment")
# Initialize the model
model = AutoModelForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment", num_labels = 3)

from datasets import load_dataset, DatasetDict
tweets = load_dataset('chiarab/final-train', use_auth_token=True)

print(tweets)

# Prepare the small dataset - this tokenizes the dataset in batches of 16 examples.
finetune_small = DatasetDict(
    train=tweets['train'].shuffle(seed=1111).select(range(128)),
    val=tweets['train'].shuffle(seed=1111).select(range(128, 160)),
    test=tweets['train'].shuffle(seed=1111).select(range(160, 260))
)

small_tokenized_dataset = finetune_small.map(
    lambda example: tokenizer(example['0'], padding=True),
    batched=True,
    batch_size=16
)

small_tokenized_dataset = small_tokenized_dataset.remove_columns(["0"])
small_tokenized_dataset = small_tokenized_dataset.rename_column("1", "labels")
small_tokenized_dataset.set_format("torch")

print(small_tokenized_dataset)

# Prepare the small dataset - this tokenizes the dataset in batches of 16 examples.
finetune = DatasetDict(
    train=tweets['train'].shuffle(seed=1111).select(range(2219)),
    val=tweets['train'].shuffle(seed=1111).select(range(2219, 2695)),
    test=tweets['train'].shuffle(seed=1111).select(range(2695, 3170))
)

tokenized_dataset = finetune.map(
    lambda example: tokenizer(example['0'], padding=True),
    batched=True,
    batch_size=16
)

tokenized_dataset = tokenized_dataset.remove_columns(["0"])
tokenized_dataset = tokenized_dataset.rename_column("1", "labels")
tokenized_dataset.set_format("torch")

print(tokenized_dataset)

from transformers import TrainingArguments, Trainer
import numpy as np

arguments = TrainingArguments(
    output_dir="sample_hf_trainer",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    evaluation_strategy="epoch", # run validation at the end of each epoch
    save_strategy="epoch",
    learning_rate=2e-5,
    load_best_model_at_end=True,
    seed=224
)


def compute_metrics(eval_pred):
    """Called at the end of validation. Gives accuracy"""
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    print("predictions")
    print(predictions)
    print("labels")
    print(labels)
    # calculates the accuracy
    return {"accuracy": np.mean(predictions == labels)}


trainer = Trainer(
    model=model,
    args=arguments,
    train_dataset=tokenized_dataset['train'],
    eval_dataset=tokenized_dataset['val'], # change to test when you do your final evaluation!
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

results = trainer.predict(tokenized_dataset['test'])

results



correct = 0
wrong = 0

# Initialize the regular tokenizer
tokenizer2 = AutoTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment")
# Initialize the regular model
model2 = AutoModelForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment", num_labels = 3)

model2.eval()
for tweet in finetune_test:
  inputs = tweet['OriginalTweet']
  tokenized_inputs = tokenizer2(inputs, return_tensors="pt")
  outputs = model2(**tokenized_inputs)
  labels = [0, 1, 2]
  prediction = torch.argmax(outputs.logits)
  if prediction == tweet['Sentiment']:
    correct += 1
  else:
    wrong += 1

print("Regular Accuracy:")
print(correct / (correct + wrong))
print(correct + wrong)

model.eval()
neg = 0
neu = 0
pos = 0

for tweet in tweets_vaccine:
  inputs = tweet['OriginalTweet']
  tokenized_inputs = tokenizer(inputs, return_tensors="pt")
  outputs = model(**tokenized_inputs)
  labels = [0, 1, 2]
  prediction = torch.argmax(outputs.logits)
  if prediction == 0:
    neg += 1
  elif prediction == 1:
    neu += 1
  else:
    pos += 1

print(neg)
print(neu)
print(pos)

print(prediction)